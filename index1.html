<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>All-in-One Voice Assistant</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .status-pulse {
            animation: pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite;
        }
        @keyframes pulse {
            0%, 100% {
                opacity: 1;
            }
            50% {
                opacity: .5;
            }
        }
    </style>
</head>
<body class="bg-gray-900 text-white flex flex-col items-center justify-center min-h-screen p-4">

    <div class="w-full max-w-3xl text-center">
        <h1 class="text-4xl md:text-5xl font-bold mb-2 bg-clip-text text-transparent bg-gradient-to-r from-blue-400 to-purple-500">Voice Assistant</h1>
        <p id="status" class="text-lg text-gray-400 mb-6 h-6">Idle</p>

        <button id="recordButton" class="px-8 py-4 text-white font-bold rounded-full transition-all duration-300 focus:outline-none focus:ring-4 focus:ring-blue-500/50 shadow-lg bg-blue-600 hover:bg-blue-700">
            Start Recording
        </button>

        <div class="mt-8 p-6 bg-gray-800 bg-opacity-50 rounded-lg shadow-md w-full text-left">
            <h2 class="text-2xl font-semibold mb-2 text-gray-200">Transcript</h2>
            <p id="transcript" class="text-gray-300 min-h-[2rem]">...</p>
        </div>

        <div class="mt-4 p-6 bg-gray-800 bg-opacity-50 rounded-lg shadow-md w-full text-left">
            <h2 class="text-2xl font-semibold mb-2 text-gray-200">Response</h2>
            <p id="response" class="text-gray-300 min-h-[2rem]">...</p>
        </div>

        <div class="mt-4 p-6 bg-gray-800 bg-opacity-50 rounded-lg shadow-md w-full text-left">
            <h2 class="text-2xl font-semibold mb-2 text-gray-200">Latencies (ms)</h2>
            <pre id="latencies" class="text-left text-gray-300 bg-gray-900 p-4 rounded min-h-[8rem]">{
  "stt": 0,
  "api": 0,
  "tts": 0,
  "playback": 0
}</pre>
        </div>
    </div>

    <script type="module">
        // Import the Transformers.js library
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1';

        // --- DOM Element References ---
        const recordButton = document.getElementById('recordButton');
        const statusEl = document.getElementById('status');
        const transcriptEl = document.getElementById('transcript');
        const responseEl = document.getElementById('response');
        const latenciesEl = document.getElementById('latencies');

        // --- State Management ---
        let isRecording = false;
        let mediaRecorder;
        let audioChunks = [];
        let latencies = { stt: 0, api: 0, tts: 0, playback: 0 };
        let sttStartTime, apiStartTime, ttsStartTime, playbackStartTime;
        let whisperModelReady = false;
        let ttsModelReady = false;

        // --- Web Worker Code as Strings ---
        const whisperWorkerCode = `
            import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1';
            env.allowLocalModels = false;

            class WhisperPipeline {
                static task = 'automatic-speech-recognition';
                static model = 'Xenova/whisper-tiny.en';
                static instance = null;

                static async getInstance(progress_callback = null) {
                    if (this.instance === null) {
                        this.instance = await pipeline(this.task, this.model, { progress_callback });
                    }
                    return this.instance;
                }
            }

            self.onmessage = async (event) => {
                const { audio } = event.data;
                try {
                    const transcriber = await WhisperPipeline.getInstance(progress => self.postMessage(progress));
                    const output = await transcriber(audio, { chunk_length_s: 30, stride_length_s: 5 });
                    self.postMessage({ status: 'complete', output });
                } catch (error) {
                    self.postMessage({ status: 'error', error: error.message });
                }
            };
        `;

        const ttsWorkerCode = `
            import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1';
            env.allowLocalModels = false;

            class TTSPipeline {
                static task = 'text-to-speech';
                static model = 'Xenova/speecht5_tts';
                static vocoder = 'Xenova/speecht5_vocoder';
                static instance = null;

                static async getInstance(progress_callback = null) {
                    if (this.instance === null) {
                        this.instance = await pipeline(this.task, this.model, { vocoder: this.vocoder, progress_callback });
                    }
                    return this.instance;
                }
            }

            self.onmessage = async (event) => {
                const { text } = event.data;
                try {
                    const synthesizer = await TTSPipeline.getInstance(progress => self.postMessage(progress));
                    const speaker_embeddings = 'https://huggingface.co/datasets/Xenova/cmu-arctic-xvectors/resolve/main/slt_arctic_xvector.bin';
                    const result = await synthesizer(text, { speaker_embeddings });
                    self.postMessage({ status: 'complete', audio: result.audio });
                } catch (error) {
                    self.postMessage({ status: 'error', error: error.message });
                }
            };
        `;

        // --- Worker Initialization ---
        // Create workers from the string code using Blobs, and specify type: 'module'
        const whisperWorker = new Worker(URL.createObjectURL(new Blob([whisperWorkerCode], { type: 'application/javascript' })), { type: 'module' });
        const ttsWorker = new Worker(URL.createObjectURL(new Blob([ttsWorkerCode], { type: 'application/javascript' })), { type: 'module' });
        
        // --- UI Update Functions ---
        function updateStatus(text, isPulsing = false) {
            statusEl.textContent = text;
            if (isPulsing) {
                statusEl.classList.add('status-pulse');
            } else {
                statusEl.classList.remove('status-pulse');
            }
        }
        
        function updateLatencies() {
            latenciesEl.textContent = JSON.stringify(latencies, null, 2);
        }

        // --- Worker Message Handlers ---
        whisperWorker.onmessage = (event) => {
            const { status, output, error, progress, file } = event.data;
            switch (status) {
                case 'complete':
                    whisperModelReady = true;
                    const sttEndTime = performance.now();
                    latencies.stt = Math.round(sttEndTime - sttStartTime);
                    updateLatencies();
                    transcriptEl.textContent = output.text;
                    updateStatus('Transcribed. Sending to AI...', true);
                    sendToAI(output.text);
                    break;
                case 'error':
                    updateStatus(`Whisper Error: ${error}`);
                    break;
                case 'progress':
                    const sttMessage = whisperModelReady ? 'Transcribing' : `Loading STT Model: ${file}`;
                    updateStatus(`${sttMessage} (${Math.round(progress || 0)}%)`, true);
                    break;
            }
        };

        ttsWorker.onmessage = (event) => {
            const { status, audio, error, progress, file } = event.data;
            switch (status) {
                case 'complete':
                    ttsModelReady = true;
                    const ttsEndTime = performance.now();
                    latencies.tts = Math.round(ttsEndTime - ttsStartTime);
                    updateLatencies();
                    playAudio(audio);
                    break;
                case 'error':
                    updateStatus(`TTS Error: ${error}`);
                    break;
                case 'progress':
                    const ttsMessage = ttsModelReady ? 'Synthesizing' : `Loading TTS Model: ${file}`;
                    updateStatus(`${ttsMessage} (${Math.round(progress || 0)}%)`, true);
                    break;
            }
        };

        // --- Core Logic ---
        async function startRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                mediaRecorder.ondataavailable = (event) => audioChunks.push(event.data);
                mediaRecorder.onstop = processAudio;
                mediaRecorder.start();
                
                isRecording = true;
                recordButton.textContent = 'Stop Recording';
                recordButton.classList.replace('bg-blue-600', 'bg-red-600');
                recordButton.classList.replace('hover:bg-blue-700', 'hover:bg-red-700');
                updateStatus('Recording...', true);
                
                transcriptEl.textContent = '...';
                responseEl.textContent = '...';
                latencies = { stt: 0, api: 0, tts: 0, playback: 0 };
                updateLatencies();

            } catch (err) {
                updateStatus('Microphone access denied.');
                console.error("Error starting recording:", err);
            }
        }

        function stopRecording() {
            mediaRecorder.stop();
            isRecording = false;
            recordButton.textContent = 'Start Recording';
            recordButton.classList.replace('bg-red-600', 'bg-blue-600');
            recordButton.classList.replace('hover:bg-red-700', 'hover:bg-blue-700');
            updateStatus('Processing audio...', true);
        }

        async function processAudio() {
            try {
                const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                const arrayBuffer = await audioBlob.arrayBuffer();
                const audioContext = new AudioContext();
                const decodedAudio = await audioContext.decodeAudioData(arrayBuffer);
                const audio = decodedAudio.getChannelData(0);

                audioChunks = [];

                updateStatus('Transcribing...', true);
                sttStartTime = performance.now();
                whisperWorker.postMessage({ audio });
            } catch (error) {
                console.error("Error processing audio:", error);
                updateStatus(`Error: Could not process audio. ${error.message}`);
            }
        }

        async function sendToAI(text) {
            apiStartTime = performance.now();
            updateStatus('Thinking...', true);
            
            await new Promise(resolve => setTimeout(resolve, 1000));
            const mockResponse = `This is a simulated response to your prompt: "${text}"`;
            
            const apiEndTime = performance.now();
            latencies.api = Math.round(apiEndTime - apiStartTime);
            updateLatencies();
            
            responseEl.textContent = mockResponse;
            updateStatus('Synthesizing audio...', true);
            ttsStartTime = performance.now();
            ttsWorker.postMessage({ text: mockResponse });
        }

        function playAudio(audioData) {
            updateStatus('Playing audio...');
            playbackStartTime = performance.now();
            
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
            const buffer = audioContext.createBuffer(1, audioData.length, 24000);
            buffer.getChannelData(0).set(audioData);

            const source = audioContext.createBufferSource();
            source.buffer = buffer;
            source.connect(audioContext.destination);
            source.start();
            
            source.onended = () => {
                const playbackEndTime = performance.now();
                latencies.playback = Math.round(playbackEndTime - playbackStartTime);
                updateLatencies();
                updateStatus('Idle');
            };
        }

        // --- Event Listener ---
        recordButton.addEventListener('click', () => {
            if (isRecording) {
                stopRecording();
            } else {
                startRecording();
            }
        });

    </script>
</body>
</html>
